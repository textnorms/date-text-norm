{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5-V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/textnorms/date_text_norm/blob/master/T5_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IFytVZT1Gbl",
        "colab_type": "code",
        "outputId": "716342e8-8039-4326-86a6-939cc42e2725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install -q num2words transformers\n",
        "!rm -rf *\n",
        "!git clone https://github.com/textnorms/date_text_norm.git\n",
        "!cp -r date_text_norm/syntetic_data/ ."
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'date_text_norm'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 35 (delta 13), reused 27 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcMfqlK-0HLY",
        "colab_type": "code",
        "outputId": "66be654b-e3e5-4478-aeba-e612c65cd4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 25 00:26:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    33W / 250W |   6691MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nok2mtt_1021",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Synthetic data generator\n",
        "from syntetic_data import DateTextGenerator\n",
        "\n",
        "# PyTorch\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhdTvVgiYBw0",
        "colab_type": "text"
      },
      "source": [
        "### Função para reproduzir resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm7tAsiUYMOU",
        "colab_type": "code",
        "outputId": "622dd9ad-e8be-40c8-d4ad-fab0c018cb16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "manual_seed = 0\n",
        "def deterministic(rep=True):\n",
        "    if rep:\n",
        "        np.random.seed(manual_seed)\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Deterministic experiment, seed: {manual_seed}')\n",
        "    else:\n",
        "        print('Random experiment')\n",
        "\n",
        "deterministic()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deterministic experiment, seed: 0\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yeoFiMYXe8Z",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9p0tFUkXiE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_size = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ52EJP6Fy5v",
        "colab_type": "code",
        "outputId": "538f02ca-11c8-423d-8407-be63a048adb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "datas = DateTextGenerator(start_date='01/01/1900',end_date='31/12/2020')\n",
        "datas.generate_demo(date='28/05/2020')"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gerando demostração dos formatos de datas geradas para a canônica: 28/05/2020\n",
            "Método: 1 --- vinte e oito do cinco de dois mil e vinte\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Método: 2 --- 28.05.2020\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Método: 3 --- vinte e oito de maio de dois mil e vinte\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Método: 4 --- vinte e oito de mai de dois mil e vinte\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBtXCuqEnQoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8eda4f8d-d92d-4a65-cdee-cde108c80b64"
      },
      "source": [
        "df = datas.generate_date_dataset(); df"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tipo padrão</th>\n",
              "      <th>Entrada</th>\n",
              "      <th>Canônico</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>um do um de mil, novecentos</td>\n",
              "      <td>01/01/1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dois do um de mil, novecentos</td>\n",
              "      <td>02/01/1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>três do um de mil, novecentos</td>\n",
              "      <td>03/01/1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>quatro do um de mil, novecentos</td>\n",
              "      <td>04/01/1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>cinco do um de mil, novecentos</td>\n",
              "      <td>05/01/1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176775</th>\n",
              "      <td>4</td>\n",
              "      <td>vinte e sete de dez de dois mil e vinte</td>\n",
              "      <td>27/12/2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176776</th>\n",
              "      <td>4</td>\n",
              "      <td>vinte e oito de dez de dois mil e vinte</td>\n",
              "      <td>28/12/2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176777</th>\n",
              "      <td>4</td>\n",
              "      <td>vinte e nove de dez de dois mil e vinte</td>\n",
              "      <td>29/12/2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176778</th>\n",
              "      <td>4</td>\n",
              "      <td>trinta de dez de dois mil e vinte</td>\n",
              "      <td>30/12/2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176779</th>\n",
              "      <td>4</td>\n",
              "      <td>trinta e um de dez de dois mil e vinte</td>\n",
              "      <td>31/12/2020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>176780 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Tipo padrão                                  Entrada    Canônico\n",
              "0                1              um do um de mil, novecentos  01/01/1900\n",
              "1                1            dois do um de mil, novecentos  02/01/1900\n",
              "2                1            três do um de mil, novecentos  03/01/1900\n",
              "3                1          quatro do um de mil, novecentos  04/01/1900\n",
              "4                1           cinco do um de mil, novecentos  05/01/1900\n",
              "...            ...                                      ...         ...\n",
              "176775           4  vinte e sete de dez de dois mil e vinte  27/12/2020\n",
              "176776           4  vinte e oito de dez de dois mil e vinte  28/12/2020\n",
              "176777           4  vinte e nove de dez de dois mil e vinte  29/12/2020\n",
              "176778           4        trinta de dez de dois mil e vinte  30/12/2020\n",
              "176779           4   trinta e um de dez de dois mil e vinte  31/12/2020\n",
              "\n",
              "[176780 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WJfgr1VblzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data \n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "        df['Entrada'].values,\n",
        "        df['Canônico'].values,\n",
        "        shuffle=True, \n",
        "        test_size=0.3, \n",
        "        random_state=manual_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsMXnkCyrVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------\n",
        "max_len_source = 40\n",
        "max_len_target = 12\n",
        "# -------------------\n",
        "\n",
        "class DateDataset(Dataset):\n",
        "    def __init__(self, data, label, tokenizer, source_max_length, target_max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.source_max_length = source_max_length\n",
        "        self.target_max_length = target_max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        source = self.data[idx]\n",
        "        target = self.label[idx]\n",
        "\n",
        "        source_tokenized = self.tokenizer.encode_plus(\n",
        "            f'{source} {self.tokenizer.eos_token}',\n",
        "            max_length=self.source_max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        target_tokenized = self.tokenizer.encode_plus(\n",
        "            f'{target} {self.tokenizer.eos_token}',\n",
        "            max_length=self.target_max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        source_token_ids = source_tokenized['input_ids'].squeeze()\n",
        "        source_mask = source_tokenized['attention_mask'].squeeze()\n",
        "        target_token_ids = target_tokenized['input_ids'].squeeze()\n",
        "        \n",
        "        return source_token_ids, source_mask, target_token_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD",
        "colab_type": "text"
      },
      "source": [
        "## Teste da classe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP",
        "colab_type": "code",
        "outputId": "a3b819fa-0180-4d47-d3f3-ad0cbfd5a575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset_debug = DateDataset(\n",
        "    x_train, \n",
        "    y_train,\n",
        "    tokenizer,\n",
        "    max_len_source,\n",
        "    max_len_target,\n",
        "    )\n",
        "\n",
        "dataloader_debug = DataLoader(\n",
        "    dataset_debug, \n",
        "    batch_size=1, \n",
        "    shuffle=True, \n",
        "    num_workers=0\n",
        "    )\n",
        "\n",
        "source_token_ids, source_mask, target_token_ids = next(iter(dataloader_debug))\n",
        "print(f'source_token_ids:\\n {source_token_ids} --- shape:{source_token_ids.shape}')\n",
        "print(f'source_mask:\\n {source_mask} --- shape:{source_mask.shape}')\n",
        "print(f'target_token_ids:\\n {target_token_ids} --- shape:{target_token_ids.shape}')"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source_token_ids:\n",
            " tensor([[ 8013, 12900,  9887,     1,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]) --- shape:torch.Size([1, 40])\n",
            "source_mask:\n",
            " tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) --- shape:torch.Size([1, 40])\n",
            "target_token_ids:\n",
            " tensor([[  586, 31497,  9887,     1,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]]) --- shape:torch.Size([1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG1O12UoWYaM",
        "colab_type": "text"
      },
      "source": [
        "## Datasets e Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AlRyeOW8GN",
        "colab_type": "code",
        "outputId": "c0e2bfce-41b6-4421-f929-cc6745039c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SZ = 128\n",
        "\n",
        "# datasets\n",
        "ds_debug = DateDataset(\n",
        "    x_train[:BATCH_SZ], \n",
        "    y_train[:BATCH_SZ],\n",
        "    tokenizer,\n",
        "    max_len_source,\n",
        "    max_len_target\n",
        "    )\n",
        "\n",
        "ds_train = DateDataset(\n",
        "    x_train, \n",
        "    y_train,\n",
        "    tokenizer,\n",
        "    max_len_source,\n",
        "    max_len_target\n",
        "    )\n",
        "ds_valid = DateDataset(\n",
        "    x_val, \n",
        "    y_val,\n",
        "    tokenizer,\n",
        "    max_len_source,\n",
        "    max_len_target\n",
        "    )\n",
        "\n",
        "# dataloaders\n",
        "dataloaders = {\n",
        "    'debug': DataLoader(\n",
        "         ds_debug,\n",
        "         batch_size=BATCH_SZ,\n",
        "         shuffle=True,\n",
        "         num_workers=2,\n",
        "         pin_memory=True),\n",
        "    \n",
        "    'train': DataLoader(\n",
        "         ds_train,\n",
        "         batch_size=BATCH_SZ,\n",
        "         shuffle=True,\n",
        "         num_workers=2,\n",
        "         pin_memory=True),\n",
        "\n",
        "    'valid': DataLoader(\n",
        "         ds_valid,\n",
        "         batch_size=BATCH_SZ,\n",
        "         shuffle=False,\n",
        "         num_workers=2,\n",
        "         pin_memory=True)\n",
        "               }\n",
        "\n",
        "# sanity check\n",
        "dl_sizes = {x: len(dataloaders[x]) for x in dataloaders.keys()}; dl_sizes "
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'debug': 1, 'train': 967, 'valid': 415}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3rqg6r7am-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testando o dataloader \n",
        "source_token_ids, source_mask, target_token_ids = next(iter(dataloaders['train']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg76FrNgw41X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, train=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_size)\n",
        "        self.training = train\n",
        "    \n",
        "    def forward(self, token_ids, att_mask, labels=None):\n",
        "        if self.training:\n",
        "            outputs = self.model.forward(\n",
        "                input_ids=token_ids, \n",
        "                attention_mask=att_mask,\n",
        "                lm_labels=labels)\n",
        "            return outputs[0] # loss\n",
        "        else:\n",
        "            predict = self.model.generate(\n",
        "                input_ids=token_ids, \n",
        "                attention_mask=att_mask,\n",
        "                max_length=max_len_target)\n",
        "            return predict\n",
        "\n",
        "    def generate_seq(self,text_input,tokenizer,max_len_source=max_len_source):\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        sample_tokenized = tokenizer.encode_plus(\n",
        "            f'{text_input} {tokenizer.eos_token}',\n",
        "            max_length=max_len_source,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "    \n",
        "            \n",
        "        sample_token_ids = sample_tokenized['input_ids']\n",
        "        sample_mask = sample_tokenized['attention_mask']\n",
        "\n",
        "        predicted_samples = self.model.generate(\n",
        "          input_ids=sample_token_ids.to(device), \n",
        "          attention_mask=sample_mask.to(device),\n",
        "          max_length=max_len_target\n",
        "          )\n",
        "        \n",
        "        self.model.train()\n",
        "\n",
        "        out_text = [tokenizer.decode(text) for text in predicted_samples]\n",
        "        \n",
        "        return out_text\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liQe2u6W4buZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eaa72adf-caae-40ad-a735-6c9ef21244d2"
      },
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "sample = 'oi mundo'\n",
        "print(f'amostra: {sample} ---- saída de amostra: {model.generate_seq(sample,tokenizer)}')\n",
        "del model"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amostra: oi mundo ---- saída de amostra: ['oi mundo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq9AMCaNuWMU",
        "colab_type": "text"
      },
      "source": [
        "## Funções de treino e eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X6hklAUXxGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc_in_text(trues, preds):\n",
        "    acc = []\n",
        "    for d in zip(trues, preds):\n",
        "        if d[0] == d[1]:\n",
        "            acc.append(1)\n",
        "        else:\n",
        "            acc.append(0)\n",
        "    return acc # bool\n",
        "\n",
        "def train(model, device, train_loader, optimizer):\n",
        "    model.train()\n",
        "    loss_train = []\n",
        "    \n",
        "    for source_token_ids, source_mask, target_token_ids in train_loader:\n",
        "        source_token_ids, source_mask, target_token_ids = source_token_ids.to(device), \\\n",
        "        source_mask.to(device), target_token_ids.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(\n",
        "            source_token_ids, \n",
        "            source_mask, \n",
        "            target_token_ids)\n",
        "        loss_train.append(loss.item())\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "  \n",
        "    ave_train_loss = sum(loss_train) / len(loss_train)\n",
        "    return ave_train_loss\n",
        "\n",
        "def evaluate_fn(model, device, val_loader):\n",
        "    model.eval()\n",
        "    model.training=False # generate  \n",
        "    all_acc, all_preds, all_trues = [], [], []\n",
        "    for source_token_ids, source_mask, target_token_ids in val_loader:\n",
        "        source_token_ids, source_mask, target_token_ids = source_token_ids.to(device), \\\n",
        "        source_mask.to(device), target_token_ids.to(device)\n",
        "        \n",
        "        predicted_tokens = model(\n",
        "            source_token_ids, \n",
        "            source_mask,\n",
        "            target_token_ids)\n",
        "        \n",
        "        preds = [tokenizer.decode(t) for t in predicted_tokens]\n",
        "        trues = [tokenizer.decode(t) for t in target_token_ids]\n",
        "        acc = acc_in_text(trues, preds)\n",
        "        all_acc.extend(acc)\n",
        "        all_preds.extend(preds)\n",
        "        all_trues.extend(trues)\n",
        "        \n",
        "    return np.array(all_acc).mean(), all_trues, all_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pirS1mecELqp",
        "colab_type": "text"
      },
      "source": [
        "# Overfit em poucas amostras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2PxRYyfn_UO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overfit = False\n",
        "if overfit:\n",
        "  start = torch.cuda.Event(enable_timing=True)\n",
        "  end = torch.cuda.Event(enable_timing=True)\n",
        "  deterministic() \n",
        "\n",
        "  model = Net(train=True).to(device)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "  epoch = 0\n",
        "  N_EPOCHS  = 1000\n",
        "\n",
        "  #--------------------------------------------------------------------------\n",
        "  start.record()\n",
        "  for step in range(1, N_EPOCHS+1):\n",
        "      loss_t = train(model, device, dataloaders['debug'], optimizer)\n",
        "      acc, trues, preds = evaluate_fn(model, device, dataloaders['debug'])\n",
        "      if step == 1:\n",
        "          print(f'[Epoch [{step}/{N_EPOCHS}] |', end=' ')\n",
        "          print(f'Train Loss: {loss_t:.3f} -- Acc: {acc:.3f}')\n",
        "      if step % 50 == 0:\n",
        "          print(f'[Epoch [{step}/{N_EPOCHS}] |', end=' ')\n",
        "          print(f'Train Loss: {loss_t:.3f} -- Acc: {acc:.3f}')\n",
        "          print(f'  Trues: {trues[:7]}\\n  Preds: {preds[:7]}')\n",
        "  end.record()\n",
        "  torch.cuda.synchronize()    \n",
        "  #--------------------------------------------------------------------------\n",
        "\n",
        "  print(f'Tempo: {start.elapsed_time(end)/1000/60 :.3f} min.')\n",
        "  del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNtIb8b4Q4lC",
        "colab_type": "code",
        "outputId": "d3a57026-2b50-471a-a288-5cc0a282d569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "deterministic() \n",
        "\n",
        "model = Net(train=True).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epoch = 0\n",
        "N_EPOCHS  = 1\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "start.record()\n",
        "for step in range(1, N_EPOCHS+1):\n",
        "    loss_t = train(model, device, dataloaders['train'], optimizer)\n",
        "    acc, trues, preds = evaluate_fn(model, device, dataloaders['valid'])\n",
        "    if step % 2 == 0:\n",
        "        print(f'  Trues: {trues[:7]}\\n  Preds: {preds[:7]}')\n",
        "    print(f'[Epoch [{step}/{N_EPOCHS}] |', end=' ')\n",
        "    print(f'Train Loss: {loss_t:.3f} -- Acc: {acc:.3f}')\n",
        "end.record()\n",
        "torch.cuda.synchronize()    \n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "print(f'Tempo: {start.elapsed_time(end)/1000/60 :.3f} min.')"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deterministic experiment, seed: 0\n",
            "[Epoch [1/1] | Train Loss: 0.686 -- Acc: 0.981\n",
            "Tempo: 5.388 min.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQPXEekhM4xS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cab830a7-7a70-4ea9-883d-7a20d2883fe7"
      },
      "source": [
        "data = 'un do janro de mil novecentu e otenta y sete'\n",
        "\n",
        "model.generate_seq(data,tokenizer)\n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['01/01/1987']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    }
  ]
}